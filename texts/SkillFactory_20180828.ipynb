{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте решим следующую задачу.<br>\n",
    "Необходимо написать робота, который будет скачивать новости с сайта Лента.Ру и фильтровать их в зависимости от интересов пользователя. От пользователя требуется отмечать интересующие его новости, по которым система будет выделять области его интересов.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с загрузки новостей. Для этого нам потребуется метод requests.get(url). Библиотека requests предоставляет серьезные возможности для загрузки информации из Интернет. Метод get получает URL стараницы и возвращает ее содержимое. В нашем случае результат будет получаться в формате html. <br>\n",
    "Загрузим необходимые библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # Загрузка новостей с сайта.\n",
    "from bs4 import BeautifulSoup # Превращалка html в текст.\n",
    "import re # Регулярные выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем загрузить страницу новостей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Response [200]>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"http://lenta.ru/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод <i>requests.get()</i> возвращает объект Response, который содержит большое количество различной информации о загруженной (или незагруженной) странице. В краткой форме отображается только результат выполения запроса. В нашем случае это 200, нет ошибки.<br> \n",
    "Посмотрим что результат содержит еще."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cookies: <RequestsCookieJar[<Cookie is_mobile=0 for .lenta.ru/>, <Cookie lid=vAsAAIveG2BsNXYRAZmIUwB= for .lenta.ru/>, <Cookie lids=483B356CF7A2730B for .lenta.ru/>]>\n",
      "time to download: 0:00:00.254351\n",
      "page encoding utf-8\n",
      "Server response:  200\n",
      "Is everything ok?  True\n",
      "Page's URL:  https://lenta.ru/news/2018/08/24/clon/\n",
      "Wall time: 274 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "resp=requests.get(\"https://lenta.ru/news/2018/08/24/clon/\")\n",
    "print(\"cookies:\", resp.cookies)\n",
    "print(\"time to download:\", resp.elapsed)\n",
    "print(\"page encoding\", resp.encoding)\n",
    "print(\"Server response: \", resp.status_code)\n",
    "print(\"Is everything ok? \", resp.ok)\n",
    "print(\"Page's URL: \", resp.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но самое для нас интересное хранится в поле <i>text</i>, которое содержит собственно текст html-страницы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'<!DOCTYPE html><html xmlns:fb=\"http://www.facebook.com/2008/fbml\" xmlns:og=\"http://ogp.me/ns#\"><head><title>В Сибири нашли подходящих для клонирования древних животных: Наука: Наука и техника: Lenta.ru</title><meta content=\"В Сибири нашли подходящих для клонирования древних животных: Наука: Наука и техника: Lenta.ru\" name=\"title\" /><meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\" />\\n<script type=\"text/javascript\">window.NREUM||(NREUM={});NREUM.info={\"beacon\":\"bam-cell.nr-data.net\",\"errorBeacon\":\"bam-cell.nr-data.net\",\"licenseKey\":\"66a8d51230\",\"applicationID\":\"1241738\",\"transactionName\":\"J19cQUoOWA0ERBoQXhRZUUYXElwOFg==\",\"queueTime\":0,\"applicationTime\":139,\"agent\":\"\"}</script>\\n<script type=\"text/javascript\">(window.NREUM||(NREUM={})).loader_config={xpid:\"VQUGU1VRGwICUFBVBAk=\",licenseKey:\"66a8d51230\",applicationID:\"1241738\"};window.NREUM||(NREUM={}),__nr_require=function(t,e,n){function r(n){if(!e[n]){var i=e[n]={exports:{}};t[n][0].call(i.exports,function(e){var i=t[n]'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество служебной информации в странице явно превышает объем текста новости. У нас есть два пути: либо использовать библиотеку BeautyfulSoup для получения текста статьи, либо получить текст с использованием регулярных выражений.<br>\n",
    "Опробуем первый путь. Документация на библиотеку BeautyfulSoup находится <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">здесь</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'В Сибири нашли подходящих для клонирования древних животных: Наука: Наука и техника: Lenta.ru\\n\\nГлавноеРоссияМирБывший СССРЭкономикаСиловые структурыНаука и техникаКультураСпортИнтернет и СМИЦенности ПутешествияИз жизниДомСтатьиГалереиВидеоСпецпроектыМоторХочешь видеть только хорошие новости? Жми!Лента добра активирована. Это зона смеха, позитива и единорожек.Лента добра деактивирована. Добро пожаловать в реальный мир.Лента добраВсё о коронавирусеНаука и техника\\xa0ВсеНаукаЖизньКосмосОружиеИсторияТехникаГаджетыИгрыСофт\\xa0АрхивПоследние новости14:20Кремль прокомментировал санкционное давление США на\\xa0Россию14:43Названа дата суда над белорусским оппозиционером Бабарико14:40Одна из\\xa0самых известных достопримечательностей мира раскололась и\\xa0рухнула в\\xa0море14:34Тренер «Фиорентины» остался недоволен формой Кокорина14:33Яна Троянова рассталась с\\xa0режиссером Сигаревым14:33Российский школьник выкопал снежный тоннель и\\xa0оказался в\\xa0ловушке14:33Трансгендерам захотели разрешить участие в\\xa0женских соревнованиях в\\xa0США14:30Нефтяной гигант заявил о\\xa0многомиллиардных убытках14:00Рассекречена максимальная скорость автомобиля Apple13:45Познер объяснил разницу между протестами в\\xa0России и\\xa0СШАНовости партнеров 15:22, 24 августа 2018В Сибири нашли подходящих для клонирования древних животныхПерейти в «Мою Ленту»Фото: APРоссийские палеонтологи обнаружили в Якутии тушу жеребенка, возраст которой достигает 30-40 тысяч лет, а также останки мамонта с мягкими тканями. Об этом сообщается в пресс-релизе на Phys.org.Специалисты отмечают хорошее состояние тела лошади, пролежавшей в вечной мерзлоте. Таким образом, находка является потенциально пригодной для клонирования животного.У найденного ископаемого, относящегося к вымершему виду Equus lenensis, сохранились кожа, шерсть, копыта, хвост и внутренние органы. Возраст жеребенка на момент смерти составлял примерно 2-3 месяца. Причиной смерти, вероятно, является попадание в какую-то «ловушку» естественного происхождения, поскольку видимых повреждений на теле не было.У трупа были взяты образцы шерсти и биологических жидкостей для тщательного генетического анализа. По словам исследователей, на данный момент это самые хорошо сохранившиеся из всех останков древних лошадей.В 2015 году в Якутии палеонтологи нашли хорошо сохранившиеся останки двух пещерных львят. Тогда ученые заинтересовались возможностью клонирования древних животных из тканей, «законсервированных» в вечной мерзлоте.Больше важных новостей в Telegram-канале «Лента дня». Подписывайтесь!Перейти в «Мою Ленту»ОбсудитьСсылки по темеИз вечной мерзлоты извлекли древний мозг щенкаlenta.ru, 28 марта 2016В Якутии нашли кости мамонта возрастом в полмиллиона летlenta.ru, 18 сентября 2015Якутия потеряла две трети ледниковlenta.ru, 11 сентября 2015Другие материалы рубрикиНаука и техника00:0112 января«Оказались втянуты в\\xa0российскую смуту»Армия бывших пленных застряла в\\xa0России и\\xa0захватила полстраны. Она не\\xa0боялась ни\\xa0Ленина, ни\\xa0Сталина00:52 — 31 январяОбнаружена самая маленькая рептилия на\\xa0планете23:01 — 30 январяПоявилась новая гипотеза о\\xa0нулевом пациенте с\\xa0ВИЧ20:53 — 30 январяУченые раскрыли влияние коронавируса на\\xa0легкие в\\xa0первые часы зараженияНаука и техника00:04 2 декабря 2020«Трупы убитых выставляли на\\xa0всеобщее обозрение»Зачем Сталин завладел Прибалтикой и\\xa0что с\\xa0ней хотел сделать Гитлер17:42 — 30 январяВВС США раскрыли подробности «убийцы» российских С-40015:01 — 30 январяПроизводство и\\xa0полет первого серийного Су-57\\xa0показали на\\xa0видео11:55 — 30 январяОпубликован сценарий превентивного удара НАТО по\\xa0Калининградской областиНаука и техника00:0119 ноября 2020Пора на\\xa0выходSony выпустила PlayStation 5. Каким получилось новое поколение консолей и\\xa0во что на\\xa0ней играть?09:20 — 30 январяРоссийский «Панцирь» вывезут в\\xa0США на\\xa0демонтаж05:25 — 30 январяНазвана выгода американцев от\\xa0похищения российского «Панциря»04:35 — 30 января«Коалиция-СВ» выпустила шесть снарядов и\\xa0поразила одну цель всеми одновременноРоссияВ работе московского метро произошел масштабный сбойВ России создадут дивизию для охраны берегов ЧукоткиМедик назвал отличия остаточных симптомов COVID-19\\xa0от повторного заражения«Они могли стоить Ельцину политической карьеры»75\\xa0лет назад Курилы вошли в\\xa0СССР. Как Россия объявила острова своими и\\xa0чуть не\\xa0потеряла их\\xa0в 90-е?Москва закупила более 6\\xa0тысяч единиц медицинской техники«Он налил в\\xa0стакан и\\xa0достал пистолет»\\xa0Все материалыМирЯпония захотела новых переговоров по\\xa0Курилам с\\xa0РоссиейОткрытый гей впервые стал министром в\\xa0США«Это возвращение к\\xa0темным временам»Как борец за\\xa0свободу и\\xa0демократию стала пособницей геноцида и\\xa0довела Мьянму до\\xa0военного переворотаЗахватившие власть в\\xa0Мьянме военные заблокировали FacebookОкружение Байдена отказалось верить в\\xa0возможность ввести санкции против РоссииБитва за\\xa0СахаруВ Африке может вспыхнуть еще одна кровопролитная война. Какую роль в\\xa0этом сыграла Америка?Все материалыБывший СССРМир в\\xa0конце тоннеляАзербайджан обещает Карабаху светлое будущее. Почему в\\xa0Армении этому не\\xa0рады?Борщ важнее, чем ЛавраИли культурные приоритеты украинской властиДиктатура в\\xa0законеВ Киргизии завершилась революция, президентом стал бывший арестант. Куда поведет страну новый лидер?Нет словВ Киеве решились окончательно разорвать связь с\\xa0Россией. Украинцам запретят говорить по-русскиВсе материалыЭкономикаЗаморозка цен в\\xa0России не\\xa0сработалаВаше сиятельствоВ мире появился новый автогигант. Удастся ли\\xa0ему победить крупнейших конкурентов?Украина признала критическую ситуацию с\\xa0углемПечальные приключения «Николы»Правоохранители открыли новое уголовное дело против собственников производителя квасаУ россиян начали выбивать долги по-новому«С 90\\xa0бутылок из\\xa0100\\xa0не платился ни\\xa0акциз, ни\\xa0НДС»Бывший владелец холдинга «Кристалл-Лефортово»\\xa0— о\\xa0крупнейшей афере на\\xa0водочном рынкеСчетная палата проверит эффективность трат на\\xa0шоу СоловьеваВсе материалыСиловые структурыЗавершено следствие по\\xa0делу экс-губернатора Хабаровского края Сергея ФургалаНавальному заменили условный срок на\\xa0реальный и\\xa0отправили в\\xa0колонию на\\xa03,5\\xa0годаОн проведет в\\xa0заключении 2\\xa0года и\\xa08\\xa0месяцев. Суд зачел ему год домашнего арестаБоевиков из\\xa0банды Басаева отдали под суд за\\xa0нападение на\\xa0российских военныхТайны следствияЭти преступления поставили в\\xa0тупик лучших следователей страны. Десятки лет они остаются нераскрытымиКиллер рассказал об\\xa0убитой и\\xa0закопанной в\\xa0подмосковном лесу женщине-ювелире«Зал так и\\xa0ахнул»Депутата Госдумы судят за\\xa0крупнейшую в\\xa0истории России взятку. Деньги для него забирали жена и\\xa0тещаРоссийской учительнице перерезали горло на\\xa0улицеВсе материалыНаука и техникаNokia воскресит легендарный телефонОбнаружено самое эффективное средство против ОРВИОписана новая проблема Cyberpunk\\xa02077«Система-Биотех» создала выявляющие мутантные штаммы коронавируса тестыВсе материалыСпортПосле ХабибаНурмагомедов не\\xa0торопится возвращаться в\\xa0UFC. Кто станет новым королем ММА?Охота на «красных дьяволов»Суперматч МЮ\\xa0с «Арсеналом» и\\xa0шанс для «Сити» упрочить лидерство — главные интриги тура АПЛ«Это был ад»Во Франции тренер годами насиловал юных учениц и\\xa0снимал их\\xa0на видео. Почему этого никто не\\xa0замечал?Британский экваторБитва Моуринью с\\xa0Клоппом и\\xa0первый матч «Челси» после отставки Лэмпарда — главные интриги тура АПЛВсе материалыКультураАктера Александра Збруева госпитализировали с\\xa0пневмониейКровавая ЛунаСоветский космос, падение Европы и\\xa0охотница на\\xa0каннибалов: главные сериалы февраляВ США скрыли фильмы о\\xa0биржевой торговле после мести юзеров Reddit инвестфондам«Он снимал с\\xa0них кожу, исследуя мышцы и\\xa0кости»Как фанатичная тяга к\\xa0вскрытию трупов помогла Микеланджело создать легендарного Давида Популярный рэпер вставит в\\xa0лоб бриллиант почти за\\xa0два миллиарда рублейМожно плакатьРаботала с\\xa0Мадонной, скрывала личность и\\xa0выпустила революционный альбом\\xa0— умерла артистка SophieРоссийскому режиссеру пообещали расправу при провале сериала The Last of\\xa0UsВсе материалыИнтернет и СМИПознер объяснил разницу между протестами в\\xa0России и\\xa0СШАДело благородноеВ России взялись за\\xa0поддержку благотворительности. Как это отразится на\\xa0тех, кому нужна помощь?У границ России заметили американские киберподразделенияПользователей WhatsApp предупредили о\\xa0новой угрозе безопасности«Я покажу тебе, кто босс этой качалки»Как низкопробное гей-порно из\\xa090-х покорило весь мир и\\xa0превратилось в\\xa0целую вселеннуюБывшие участницы «Последнего героя» рассказали о\\xa0закулисье передачи«Учат весь мир свободе слова»\\xa0Все материалыЦенностиКим Кардашьян вернула в\\xa0моду прическу из\\xa090-х«Русской женщине плохо выглядеть нельзя»Россиянки привыкли быть безупречными. Как\\xa0отечественные салоны красоты стали лучшими в\\xa0мире?Беременная невеста российского миллиардера снялась топлес в\\xa0спущенных джинсахТолько для избранныхРоскошь, дешевый алкоголь и\\xa0никаких женщин: как появились элитные мужские клубыДевушка показала женскую одежду на\\xa0спящем муже и\\xa0рассмешила покупателейРуки прочьЛюбимая вещь европейских богачей вернулась в\\xa0моду. 200\\xa0лет назад в\\xa0России она была под запретомДевушка похудела и\\xa0прославилась из-за сходства с\\xa0известной модельюВсе материалыПутешествияБелая смертьСоль, женский труд и\\xa0проституция. Чем живут селения на\\xa0берегах самых крупных озер Африки?Лавина обрушилась на\\xa0людей в\\xa0Домбае. Есть погибшийЧто известно о\\xa0происшествии на\\xa0популярном горнолыжном курорте России«Местные не\\xa0осуждают любовь за\\xa0деньги»Шашлычные, цыганки и\\xa0русские в\\xa0джунглях: что искать туристу в\\xa0Колумбии?Кровавая крепостьРасстрелы, отравления и\\xa0чума: что творилось за\\xa0стенами замка исламских правителейВсе материалыИз жизниРазбитая фара на\\xa0машине спасла девушке жизньПокупка сэндвича обогатила мужчину на\\xa011\\xa0миллионов рублей«Швейцарцы говорят, что русская речь звучит как музыка»История девушки из\\xa0России, основавшей университет в\\xa0ШвейцарииЮноша показал простой способ отделить белок от\\xa0желтка с\\xa0помощью чеснокаКокаиновое наследиеБегемоты главного наркобарона мира Эскобара захватили Колумбию. Никто не\\xa0знает, как их\\xa0остановитьЖена потратила миллионы рублей на\\xa0пластические операции после комментария мужа«Как будто началась война»В американском городке решили построить утопию без налогов и\\xa0законов. Все испортили дикие звериВсе материалыАвтоВАЗ объявил о\\xa0старте производства обновленного Lada LargusВдохновленный пустыней: Ford представил новый F-150\\xa0Raptor«Горячая» версия пикапа в\\xa0третьем поколении нарастила внедорожный потенциалВ России сфотографировали Toyota Land Cruiser нового поколенияПервый тест 8-местного кроссовера Hyundai PalisadeЗнакомимся с\\xa0новой моделью Hyundai — флагманским кроссовером Palisade — на\\xa0российских дорогахРоссия и\\xa0Белоруссия могут вместе построить электрокарНовый DS\\xa04\\xa0составит конкуренцию младшим моделям BMW и\\xa0AudiХэтчбек полностью изменился внешне и\\xa0получил уникальное для сегмента оснащениеApple наняла инженера из\\xa0Porsche для работы над своим первым электрокаромВсе материалыДом«Моя жизнь радикально преобразилась за\\xa0месяц»История россиянина, решившего обменять скрепку на\\xa0квартируНе сразу строиласьЕе внешний вид определял лично Сталин: как появилась главная гостиница МосквыЛавочка закрываетсяВ России запретят строить апартаменты. Что ждет владельцев «неправильного» жилья?Едим домаВид на\\xa0море, дыра в\\xa0стене и\\xa0куклы Барби: как выглядят самые современные кухни планеты?Все материалыНацпроектыПутин повысил возраст молодежиМассовое кодированиеВ России появятся цифровые рубли. Как новая валюта изменит экономику?Школьные аттестаты захотели промаркировать QR-кодами«Объем домашней работы вырос на\\xa0250\\xa0процентов»Что думают о\\xa0школьной удаленке родители, дети и\\xa0учителя?Приамурье впервые получит финансирование на\\xa0региональные дороги по\\xa0нацпроектуИдите на\\xa0светРоссияне годами скрывали свои доходы. Как теперь их\\xa0заставляют платить налоги?Российский поселок полностью избавился от\\xa0аварийного жильяВсе материалы \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nМобильная версияНашли опечатку?Нажмите Ctrl+EnterОзвучка МатериаловРедакцияРекламаПресс-релизыТехподдержкаСпецпроектыВакансииRSS© 1999–2021 ООО «Лента.Ру».'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BeautifulSoup(resp.text, \"html.parser\").get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, убрать html-теги получилось. Но их содержимое осталось, в том числе и скрипты.<br>\n",
    "Опробуем другой путь. Весь текст обычно оформляется тегом параграфа - &lt;p&gt;. Выберем весь текст из этих тегов. Заодно выберем и заголовок статьи, оформленный при помощи <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В Сибири нашли подходящих для клонирования древних животных \n",
      "-----\n",
      " Российские палеонтологи обнаружили в Якутии тушу жеребенка, возраст которой достигает 30-40 тысяч лет, а также останки мамонта с мягкими тканями. Об этом сообщается в пресс-релизе на Phys.org. Специалисты отмечают хорошее состояние тела лошади, пролежавшей в вечной мерзлоте. Таким образом, находка является потенциально пригодной для клонирования животного. У найденного ископаемого, относящегося к вымершему виду Equus lenensis, сохранились кожа, шерсть, копыта, хвост и внутренние органы. Возраст жеребенка на момент смерти составлял примерно 2-3 месяца. Причиной смерти, вероятно, является попадание в какую-то «ловушку» естественного происхождения, поскольку видимых повреждений на теле не было. У трупа были взяты образцы шерсти и биологических жидкостей для тщательного генетического анализа. По словам исследователей, на данный момент это самые хорошо сохранившиеся из всех останков древних лошадей. В 2015 году в Якутии палеонтологи нашли хорошо сохранившиеся останки двух пещерных львят. Тогда ученые заинтересовались возможностью клонирования древних животных из тканей, «законсервированных» в вечной мерзлоте.\n"
     ]
    }
   ],
   "source": [
    "bs=BeautifulSoup(resp.text, \"html.parser\") \n",
    "title=bs.h1.text\n",
    "text=BeautifulSoup(\" \".join([p.text for p in bs.find_all(\"p\")]), \"html.parser\").get_text()\n",
    "print(title, \"\\n-----\\n\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось хорошо. Но опробуем второй путь.<br>\n",
    "Теперь попробуем использовать регулярные выражения в два шага. На первом мы вырежем только саму новость с ее оформлением используя для этого регулярные выражения (библиотека re). На втором шаге мы используем библиотеку BeautifulSoup для \"выкусывания\" тегов html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFeatureNotFound\u001B[0m                           Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-7-741e5b397b8b>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[0mtext\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdelscript\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;31m# Выкусываем остальные теги.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mBeautifulSoup\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtitle\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;34m\"\\n-----\\n\"\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"lxml\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mP:\\wd\\miniconda\\envs\\skill_factory\\lib\\site-packages\\bs4\\__init__.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001B[0m\n\u001B[0;32m    241\u001B[0m             \u001B[0mbuilder_class\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbuilder_registry\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlookup\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    242\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mbuilder_class\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 243\u001B[1;33m                 raise FeatureNotFound(\n\u001B[0m\u001B[0;32m    244\u001B[0m                     \u001B[1;34m\"Couldn't find a tree builder with the features you \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    245\u001B[0m                     \u001B[1;34m\"requested: %s. Do you need to install a parser library?\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFeatureNotFound\u001B[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "findheaders = re.compile(\"<h1.+?>(.+)</h1>\", re.S)\n",
    "boa = re.compile('<div class=\"b-text clearfix js-topic__text\" itemprop=\"articleBody\">', re.S)\n",
    "eoa = re.compile('<div class=\"b-box\">\\s*?<i>', re.S)\n",
    "delscript = re.compile(\"<script.*?>.+?</script>\", re.S)\n",
    "\n",
    "# Получает текст страницы.\n",
    "art=requests.get(\"https://lenta.ru/news/2018/08/24/clon/\")\n",
    "# Находим заголовок.\n",
    "title = findheaders.findall(art.text)[0]\n",
    "# Выделяем текст новости.\n",
    "text = eoa.split(boa.split(art.text)[1])\n",
    "# Иногда новость оканчивается другим набором тегов.\n",
    "if len(text)==1:\n",
    "    text = re.split('<div itemprop=\"author\" itemscope=\"\"', text[0])\n",
    "# Выкусываем скрипты - BeautifulSoup не справляетсяя с ними.\n",
    "text = \"\".join(delscript.split(text[0]))\n",
    "# Выкусываем остальные теги.\n",
    "print(BeautifulSoup(title+\"\\n-----\\n\"+text, \"lxml\").get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на этот фрагмент.<br>\n",
    "<i>... видимых повреждений на теле не было.У трупа были взяты образцы шерсти...</i><br>\n",
    "BeautyfulSoup именно \"выкусывает\" теги, не заменяя их на пробелы. Иногда это можжет приводить к искожению текста из-за \"склеивания\" слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"6\">Закрепим пройденный материал выполнением небольшого задания.</text>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь напишем функцию, которая выгружает все новости за сутки. <br>\n",
    "Обратим внимание, что для сайта Lenta.ru можно написать адрес в формате lenta.ru/ГГГГ/ММ/ДД/ (год, месяц, день) и получить все новости за этот день. Попробуем получить все адреса с такой страницы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup(requests.get(\"http://lenta.ru/2018/08/25/\").text, \"html.parser\").find_all(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, это опять немного не то, что нам нужно. Мы получили все ссылки, находящиеся на боковом меню, ссылки на события сегодняшнего дня и другие ненужные нам вещи. <br>\n",
    "Смотрим в содержимое html-страницы и обращаем внимание, что все интересные нам ссылки оформлены как заголовки третьего уровня - &lt;h3&gt;. Извлечем все такие фрагменты, а потом извлечем собственно адреса, помеченные атрибутом href тега &lt;a&gt;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3s=BeautifulSoup(requests.get(\"http://lenta.ru/2020/12/25/\").text, \"html.parser\").find_all(\"h3\")\n",
    "links=[\"http://lenta.ru\"+l.find_all(\"a\")[0][\"href\"] for l in h3s]\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если теперь написать функцию, которая будет перебирать все адреса и получать из них тексты новостей, то мы получим все новости за определенные сутки. Но это мы сделаем чть позже, а пока просто оформим код загрузки статьи в виде функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка статьи по URL.\n",
    "def getOneLentaArticle(url):\n",
    "    \"\"\" getLentaArticle gets the body of an article from Lenta.ru\"\"\"\n",
    "    # Получает текст страницы.\n",
    "    resp=requests.get(url)\n",
    "    # Загружаем текст в объект типа BeautifulSoup.\n",
    "    bs=BeautifulSoup(resp.text, \"html.parser\") \n",
    "    # Получаем заголовок статьи.\n",
    "    aTitle=bs.h1.text.replace(\"\\xa0\", \" \")\n",
    "    # Получаем текст статьи.\n",
    "    anArticle=BeautifulSoup(\" \".join([p.text for p in bs.find_all(\"p\")]), \"html5lib\").get_text().replace(\"\\xa0\", \" \")\n",
    "    return aTitle, anArticle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобравшись с базовыми библиотеками, перейдем теперь к обработке собственно текстов. Самостоятельно это можно сделать прочитав одну из двух книг: <a href='https://miem.hse.ru/clschool/the_book'>поновее</a> и <a href='http://clschool.miem.edu.ru/uploads/swfupload/files/011a69a6f0c3a9c6291d6d375f12aa27e349cb67.pdf'>постарше</a> (в старой хорошо разобраны классификация и кластеризация, в новой - тематическое моделирование и рядом лежит видео лекций).<br>\n",
    "Для обработки текста проводится два этапа анализа: <b>графематический</b> (выделение предложений и слов) и <b>морфологический</b> (определение начальной формы слова, его части речи и грамматических параметров). Этап синтаксического анализа мы разбирать не будем, так как его информация требуется не всегда.<br>\n",
    "Задачей графематического анализа является разделение текста на составные части - врезки, абзацы, предложения, слова. В таких задачах как машинный перевод, точность данного этапа может существенно влиять на точность получаемых результатов. Например, точка, используемая для сокращений, может быть воспринята как конец предложения, что полность разорвет его семантику.<br>\n",
    "Но в некоторых задачах (например нашей) используется подход <b>\"мешок слов\"</b> - текст воспринимается как неупорядоченное множество слов, для которых можно просто посчитать их частотность в тексте. Данный подход проще реализовать, для него не нужно делать выделение составных частей текста, а необходимо только выделить слова.  Именно этот подход мы и будем использовать.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2 # Морфологический анализатор.\n",
    "from collections import Counter # Не считать же частоты самим.\n",
    "import math # Корень квадратный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачей морфологического анализа является определение начальной формы слова, его части речи и грамматических параметров. В некоторых случаях от слова требуется только начальная форма, в других - только начальная форма и часть речи.<br>\n",
    "Существует два больших подхода к морфологическому анализу: <b>стемминг</b> и <b>поиск по словарю</b>. Для проведения стемминга оставляется справочник всех окончаний для данного языка. Для пришедшего слова проверяется его окончание и по нему делается прогноз начальной формы и части речи.<br>\n",
    "Например, мы создаем справочник, в котором записываем все окончания прилагательных: <i>-ому, -ему, -ой, -ая, -ий, -ый, ...</i> Теперь все слова, которые имеют такое окончание будут считаться прилагаельными: <i>синий, циклический, красного, больному</i>. Заодно прилагательными будут считаться причастия (<i>делающий, строившему</i>) и местоимения (<i>мой, твой, твоему</i>). Также не понятно что делать со словами, имеющими пустое окончание. Отдельную проблему составляют такие слова, как <i>стекло, больной, вина</i>, которые могут разбираться несколькими вариантами (это явление называется <b>омонимией</b>). Помимо этого, стеммер может просто откусывать окончания, оставляя лишь псевдооснову.<br>\n",
    "Большинство проблем здесь решается, но точность работы бессловарных стеммеров находится на уровне 80%. Чтобы повысить точность испольуют морфологический анализ со словарем. Разработчики составляют словарь слов, встретившихся в текстах (<a href=\"http://opencorpora.org/dict.php\">здесь</a> можно найти пример такого словаря). Теперь каждое слово будет искаться в словаре и не предсказываться, а выдаваться точно. Для слов, отсутствующих в словаре, может применяться предсказание, пообное работе стеммера.<br>\n",
    "Посмотрим как работает словарная морфология на примере системы pymorphy2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph=pymorphy2.MorphAnalyzer() # Создает объект морфоанализатора и загружет словарь.\n",
    "wordform=morph.parse('стекло')  # Проведем анализ слова \"стекло\"...\n",
    "print(wordform)                 # ... и посмотрим на результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из вывода, слово \"стекло\" может быть неодушевленным существительным среднего рода, единственного числа, именительного падежа <i>tag=OpencorporaTag('NOUN,inan,neut sing,nomn')</i>, аналогично, но в винительном падеже (<i>'NOUN,inan,neut sing,accs'</i>), и глаголом <i>'VERB,perf,intr neut,sing,past,indc'</i>. При этом в первой форме оно встречается в 75% случаев (<i>score=0.75</i>), во второй в 18,75% случаев (<i>score=0.1875</i>), а как глагол - лишь в 6,25% (<i>score=0.0625</i>). Самым простым видом борьбы с омонимией является выбор нулевого элемента из списка, возвращенного морфологическим анализом. Такой подход дает около 90% точности при выборе начальной формы и до 80% если мы обращаем внимание на грамматические параметры.<br><br>\n",
    "Вместо Pymorphy можно использовать PyMystem. Его плюсом является тот факт, что он сам проводит графематический анализ и снимает омонимию. Используя функцию lemmatize можно получить набор начальных форм слов. Используя функцию analyze можно получить полную информацию о словах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymystem3\n",
    "mystem=pymystem3.Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mystem.lemmatize('эти типы стали есть в цеху'))\n",
    "print(mystem.analyze('эти типы стали есть в цеху'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но мы будем использовать pymorphy, так как он немного пошустрее.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_title, art_text = getOneLentaArticle(\"https://lenta.ru/news/2018/02/15/greben/\")\n",
    "print(art_title, \"\\n-----\\n\", art_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для новостной заметки можно составить ее словарь, а также посчитать частоты всех слов. В итоге мы получим представление текста в виде вектора. В этом векторе координаты будут называться по соответствующим словам, а смещение по данной координате будет показывать частота. <br>\n",
    "При составлении словаря будем учитывать только значимые слова - существительные, прилагательные и глаголы. Помимо этого предусмотрим возможность учитывать часть речи слова, прибавляя ее у начальной форме.<br>\n",
    "Для разделения текста на слова используем простейший алгоритм: слово - это последовательность букв русского алфавита среди которых может попадаться дефис. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posConv={'ADJF':'_ADJ','NOUN':'_NOUN','VERB':'_VERB'}\n",
    "meaningfullPoSes=['ADJF', 'NOUN', 'VERB']\n",
    "\n",
    "def getArticleDictionary(text, needPos=None):\n",
    "    words=[a[0] for a in re.findall(\"([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)\", text)]\n",
    "    reswords=[]\n",
    "\n",
    "    for w in words:\n",
    "        wordform=morph.parse(w)[0]\n",
    "        if wordform.tag.POS in meaningfullPoSes:\n",
    "            if needPos!=None:\n",
    "                reswords.append(wordform.normal_form+posConv[wordform.tag.POS])\n",
    "            else:\n",
    "                reswords.append(wordform.normal_form)\n",
    "            \n",
    "    return Counter(reswords)\n",
    "\n",
    "stat1=getArticleDictionary(art_text, True)\n",
    "print(stat1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более правильным методом является использование CountVectorizer из sklearn.feature_extraction.text. При помощи функции <i>fit_transform</i> можно получить разреженное представление матрицы частот слов. Основная проблема состоит в том, что индексы в матрице представляют собой индексы в словаре переданных текстов. Сам словарь хранится в свойстве <i>vocabulary_</i> и умеет возвращать индекс по слову (но не наоборот)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "counter=CountVectorizer()\n",
    "# Просим посчитать частоты слов.\n",
    "res=counter.fit_transform([art_text])\n",
    "# Разреженное представление счетчика.\n",
    "print(res[0][0,:10])\n",
    "# Можно получить индекс по слову, ...\n",
    "print(counter.vocabulary_.get('фильм'))\n",
    "# ... но не наоборот.\n",
    "print(counter.vocabulary_.get(110))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более того, CountVectorizer просто выделяет подстроки и ничего не знает про морфологию (ее можно правильно прикрутить, но это хлопотное занятие). Зато он умеет выделять n-граммы (n слов идущих подряд (или даже букв)). Помимо этого, можно попросить выдать все подстроки, создав анализатор. И можно сказать как выделять подстроки при помощи регулярного выражения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics=\"\"\"У тебя в кармане\n",
    "Два мелка и волшебный камень.\n",
    "Ты волшебный камень\n",
    "На Восьмое марта подаришь маме.\n",
    "А с высокой крыши\n",
    "Все на свете слышно.\n",
    "Кто-то хитрый и большой\n",
    "Наблюдает за тобой.\"\"\"\n",
    "\n",
    "# При помощи ngram_range=(1,2) говорим, что хотим извлекать слова и пары слов.\n",
    "# token_pattern показывает регулярное выражение, которому должны соответствовать слова.\n",
    "counter12=CountVectorizer(ngram_range=(1,2))#, token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "# Проводим анализ, получаем список найденных n-грамм.\n",
    "analyze = counter12.build_analyzer()\n",
    "print(analyze(lyrics))\n",
    "# Считаем частоты, видим, что слова не приводились к начальной форме.\n",
    "res=counter12.fit_transform([lyrics])\n",
    "print(counter12.vocabulary_.get('маме'))\n",
    "print(counter12.vocabulary_.get('мама'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем прикрутить к нему морфологию, но сделаем это немного по-своему. Разобьем строку на слова, проведем их морфологический анализ, возьмем начальные формы и снова склеим в строку, которую отдадим в CountVectorizer. И сделаем тоже самое, только для значимых слов. Как видно из результата, текст перестал быть про \"тебя\", а стал о чем-то немного другом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeaningfullWords(text):\n",
    "    words=[]\n",
    "    tokens=re.findall('[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+', text)\n",
    "    for t in tokens:\n",
    "        pv=morph.parse(t)\n",
    "        for p in pv:\n",
    "            if p.tag.POS in ['ADJF', 'NOUN', 'VERB']:\n",
    "                words.append(p.normal_form)\n",
    "                break\n",
    "    return words\n",
    "\n",
    "c=[' '.join(getMeaningfullWords(lyrics))]\n",
    "c2=[' '.join([morph.parse(r)[0].normal_form for r in re.findall('[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+', lyrics)])]\n",
    "\n",
    "lemmaCounter=CountVectorizer(ngram_range=(1,2), token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "\n",
    "analyze = lemmaCounter.build_analyzer()\n",
    "res1=analyze(c[0])\n",
    "res2=lemmaCounter.fit_transform(c)\n",
    "print({w:res2[0][0,lemmaCounter.vocabulary_[w]] for w in lemmaCounter.vocabulary_ if res2[0][0,lemmaCounter.vocabulary_[w]]>1})\n",
    "print(\"---\")\n",
    "res1=analyze(c2[0])\n",
    "res2=lemmaCounter.fit_transform(c2)\n",
    "print({w:res2[0][0,lemmaCounter.vocabulary_[w]] for w in lemmaCounter.vocabulary_ if res2[0][0,lemmaCounter.vocabulary_[w]]>1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на слово \"исполняющий\". В исходном тексте его не было.<br>\n",
    "Оно появилось так как одним из (не самых вероятных) значений слова \"и\" является сокращение от \"исполняющий\". Так как для каждого слова я пытался найти его значение как значимой части речи, союз был отброшен, а вот прилагательное осталось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но это лирика, а в новостях получается не так чувствительно. Более того, на одно из первых мест выходит устойчивое словосочетание - \"домашний арест\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=[' '.join(getMeaningfullWords(art_text))]\n",
    "c2=[' '.join([morph.parse(r)[0].normal_form for r in re.findall('[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+', art_text)])]\n",
    "\n",
    "lemmaCounter=CountVectorizer(ngram_range=(1,2), token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "\n",
    "analyze = lemmaCounter.build_analyzer()\n",
    "res1=analyze(c[0])\n",
    "res2=lemmaCounter.fit_transform(c)\n",
    "print({w:res2[0][0,lemmaCounter.vocabulary_[w]] for w in lemmaCounter.vocabulary_ if res2[0][0,lemmaCounter.vocabulary_[w]]>1})\n",
    "print(\"---\")\n",
    "res1=analyze(c2[0])\n",
    "res2=lemmaCounter.fit_transform(c2)\n",
    "print({w:res2[0][0,lemmaCounter.vocabulary_[w]] for w in lemmaCounter.vocabulary_ if res2[0][0,lemmaCounter.vocabulary_[w]]>1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но нам необходимо искать новости, которые интересны пользователю.<p>\n",
    "Для определения меры сходства двух статей теперь может использоваться косинусная мера сходства, рассчитываемая по следующей формуле: $cos(a,b)=\\frac{\\sum{a_i * b_i}}{\\sqrt {\\sum{a_i^2}*\\sum{b_i^2}}}$.<br>\n",
    "Вообще-то, использовать стандартную функцию рассчета косинусной меры сходства из <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\">sklearn</a> было бы быстрее. Но в данной задаче нам бы пришлось сводить все словари в один, чтобы на одних и тех же местах в векторе были частоты одних и тех же слов. Чтобы избежать подобной работы, напишем собственную функцию рассчета косинусного расстояния, работающую с разреженными векторами в виде питоновских словарей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(a, b):\n",
    "    if len(a.keys())==0 or len(b.keys())==0:\n",
    "        return 0\n",
    "    sumab=sum([a[na]*b[na] for na in a.keys() if na in b.keys()])\n",
    "    suma2=sum([a[na]*a[na] for na in a.keys()])\n",
    "    sumb2=sum([b[nb]*b[nb] for nb in b.keys()])\n",
    "    return sumab/math.sqrt(suma2*sumb2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кстати, вы обратили внимание на красивую формулу в маркдауне выше? Здесь можно использовать html и LaTex для красивого оформления. Например, тег &lt;b&gt;позволяет сделать текст <b>полужирным</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем значение косинусной меры для разных статей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat2=getArticleDictionary(getOneLentaArticle(\"https://lenta.ru/news/2018/02/15/pengilly_domoi/\")[1], True)\n",
    "stat3=getArticleDictionary(getOneLentaArticle(\"https://lenta.ru/news/2018/02/15/tar_mor/\")[1], True)\n",
    "stat4=getArticleDictionary(getOneLentaArticle(\"https://lenta.ru/news/2018/02/15/olympmovies/\")[1], True)\n",
    "\n",
    "print(cosineSimilarity(stat1, stat2))\n",
    "print(cosineSimilarity(stat1, stat3))\n",
    "print(cosineSimilarity(stat2, stat3))\n",
    "print(cosineSimilarity(stat2, stat4))\n",
    "print(cosineSimilarity(stat3, stat4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось, на самом деле, так себе - статьи очень слабо походят друг на друга. Но может быть потом выйдет лучше.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"6\">А теперь немного поупражняемся.</font><br>\n",
    "Напишите функцию, которая ищет процент пересечения словарей у двух статей с применением MyStem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оформим теперь весь необходимый код как класс, который будет загружать новости с сайта, сохранять их в файл и читать из предварительно сохраненного файла.<br>\n",
    "Класс - это тип, определенный пользователем (программистом). Класс содержит в себе как данные, так и функции, которые работают с этими данными.<br>\n",
    "Так как это тип, то можно создавать переменные этого типа. Каждая переменная будет хранить и обрабатывать свой набор данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getNewsPaper:\n",
    "        \n",
    "    # Конструктор - вызывается при создании объекта и инициализирует его.\n",
    "    def __init__(self):\n",
    "        self.articles=[]     # Загруженные статьи.\n",
    "        self.titles=[]       # Заголовки статей.\n",
    "        self.dictionaries=[] # Словари для каждой из статей.\n",
    "        # Создаем и загружаем морфологический словарь.\n",
    "        self.morph=pymorphy2.MorphAnalyzer()\n",
    "\n",
    "    # Загрузка статьи по URL.\n",
    "    def getLentaArticle(self, url):\n",
    "        \"\"\" getLentaArticle gets the body of an article from Lenta.ru\"\"\"\n",
    "        # Получает текст страницы.\n",
    "        resp=requests.get(url)\n",
    "        # Загружаем текст в объект типа BeautifulSoup.\n",
    "        bs=BeautifulSoup(resp.text, \"html5lib\") \n",
    "        # Получаем заголовок статьи.\n",
    "        self.titles.append(bs.h1.text.replace(\"\\xa0\", \" \"))\n",
    "        # Получаем текст статьи.\n",
    "        self.articles.append(BeautifulSoup(\" \".join([p.text for p in bs.find_all(\"p\")]), \"html5lib\").get_text().replace(\"\\xa0\", \" \"))\n",
    "\n",
    "    # Загрузка всех статей за один день.\n",
    "    def getLentaDay(self, url):\n",
    "        \"\"\" Gets all URLs for a given day and gets all texts. \"\"\"\n",
    "        try:\n",
    "            # Грузим страницу со списком всех статей.\n",
    "            day = requests.get(url) \n",
    "            # Получаем фрагменты с нужными нам адресами статей.\n",
    "            h3s=BeautifulSoup(day.text, \"html5lib\").find_all(\"h3\")\n",
    "            # Получаем все адреса на статьи за день.\n",
    "            links=[\"http://lenta.ru\"+l.find_all(\"a\")[0][\"href\"] for l in h3s]\n",
    "            # Загружаем статьи.\n",
    "            for l in links:\n",
    "                self.getLentaArticle(l)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Загрузка всех статей за несколько дней.\n",
    "    def getLentaPeriod(self, start, finish):\n",
    "        curdate=start\n",
    "        while curdate<=finish:\n",
    "            print(curdate.strftime('%Y/%m/%d')) # Just in case.\n",
    "            # Список статей грузится с вот такого адреса.\n",
    "            self.getLentaDay('https://lenta.ru/news/'+curdate.strftime('%Y/%m/%d'))\n",
    "            curdate+=datetime.timedelta(days=1)\n",
    "\n",
    "    # Потроение вектора для статьи.\n",
    "    posConv={'ADJF':'_ADJ','NOUN':'_NOUN','VERB':'_VERB'}\n",
    "    def getArticleDictionary(self, text, needPos=None):\n",
    "        words=[a[0] for a in re.findall(\"([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)\", text)]\n",
    "        reswords=[]\n",
    "    \n",
    "        for w in words:\n",
    "            wordform=self.morph.parse(w)[0]\n",
    "            try:\n",
    "                if wordform.tag.POS in ['ADJF', 'NOUN', 'VERB']:\n",
    "                    if needPos!=None:\n",
    "                        reswords.append(wordform.normal_form+self.posConv[wordform.tag.POS])\n",
    "                    else:\n",
    "                        reswords.append(wordform.normal_form)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        stat=Counter(reswords)\n",
    "#        stat={a: stat[a] for a in stat.keys() if stat[a]>1}\n",
    "        return stat\n",
    "\n",
    "    # Посчитаем вектора для всех статей.\n",
    "    def calcArticleDictionaries(self, needPos=None):\n",
    "        self.dictionaries=[]\n",
    "        for a in self.articles:\n",
    "            self.dictionaries.append(self.getArticleDictionary(a, needPos))\n",
    "            \n",
    "    # Сохраняем статьи в файл.\n",
    "    def saveArticles(self, filename):\n",
    "        \"\"\" Saves all articles to a file with a filename. \"\"\"\n",
    "        newsfile=open(filename, \"w\")\n",
    "        for art in self.articles:\n",
    "            newsfile.write('\\n=====\\n'+art)\n",
    "        newsfile.close()\n",
    "\n",
    "    # Читаем статьи из файла.\n",
    "    def loadArticles(self, filename):\n",
    "        \"\"\" Loads and replaces all articles from a file with a filename. \"\"\"\n",
    "        newsfile=open(filename, encoding=\"utf-8\")\n",
    "        text=newsfile.read()\n",
    "        self.articles=text.split('\\n=====\\n')[1:]\n",
    "        for a in self.articles:\n",
    "            self.titles.append(a.split('\\n-----\\n')[0])\n",
    "        newsfile.close()\n",
    "\n",
    "    # Для удобства - поиск статьи по ее заголовку.\n",
    "    def findNewsByTitle(self, title):\n",
    "        if title in self.titles:\n",
    "            return self.titles.index(title)\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "def cosineSimilarity(a, b):\n",
    "    if len(a.keys())==0 or len(b.keys())==0:\n",
    "        return 0\n",
    "    sumab=sum([a[na]*b[na] for na in a.keys() if na in b.keys()])\n",
    "    suma2=sum([a[na]*a[na] for na in a.keys()])\n",
    "    sumb2=sum([b[nb]*b[nb] for nb in b.keys()])\n",
    "    return sumab/math.sqrt(suma2*sumb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим статьи.<br>\n",
    "<b>!!! Настоятельно рекомендую использовать ячейку с загрузкой статей из файла !!!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка статей за заданный период.\n",
    "# !!! Это рабоатет довольно долго, пользуйтесь сохраненными данными!!!\n",
    "lenta=getNewsPaper()\n",
    "lenta.getLentaPeriod(datetime.date(2018, 2, 1), datetime.date(2018, 2, 14))\n",
    "lenta.saveArticles(\"lenta2018.txt\")\n",
    "#lenta.loadArticles(\"lenta2018.txt\")\n",
    "lenta.calcArticleDictionaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenta=getNewsPaper()\n",
    "lenta.loadArticles(\"lenta2018.txt\")\n",
    "lenta.calcArticleDictionaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из чистого любопытства попробуем найти статью, наиболее похожую на данную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Конечно же, правильнее делать это через np.argmax().\n",
    "i1 = 0\n",
    "maxCos, maxpos = -1, -1\n",
    "for i in range(len(lenta.articles)):\n",
    "    if i != i1:\n",
    "        c = cosineSimilarity(lenta.dictionaries[i1], lenta.dictionaries[i])\n",
    "        if c>maxCos:\n",
    "            maxCos, maxpos = c, i\n",
    "print(lenta.articles[i1].split('\\n-----\\n')[0])\n",
    "print(lenta.articles[maxpos].split('\\n-----\\n')[0])\n",
    "print(maxCos, maxpos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сходство между статьями достаточно велико. Есть большие шансы за то, что они об одном и том же.<br><br>\n",
    "Теперь попробуем решить основную задачу.<br>\n",
    "Пользователь выбирает несколько статей на интересующую его тематику. Пусть это будут олимпиада и выборы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likesport=['Власти США обвинили МОК и ФИФА в коррупции', 'Пробирки WADA для допинг-проб оказались бракованными', 'Пожизненно отстраненных российских спортсменов оправдали', 'В Кремле порадовались за оправданных российских спортсменов', 'Россия вернется на первое место Олимпиады-2014', 'МОК разочаровало оправдание российских олимпийцев', 'Мутко загрустил после оправдания российских спортсменов', 'Оправданный призер Сочи-2014 призвал «добить ситуацию» с МОК', 'Путин предостерег от эйфории после оправдания российских олимпийцев', 'Родченков не смог вразумительно ответить на вопросы суда', 'Оправданный россиянин позлорадствовал над делившими медали Игр-2014 иностранцами', 'В CAS отказались считать оправданных россиян невиновными', 'Адвокат Родченкова заговорил о смерти чистого спорта после оправдания россиян', 'Американская скелетонистка сочла россиян ушедшими от законного наказания']\n",
    "likeelect=['Социологи подсчитали планирующих проголосовать на выборах-2018', 'Собчак пообещала дать Трампу пару советов', 'На выборы президента России пойдут почти 80 процентов избирателей', 'Песков вспомнил предупреждение и отказался комментировать поездку Собчак в США', 'Собчак съездила на завтрак с Трампом и разочаровалась', 'Грудинин уступил в популярности КПРФ', 'Собчак потребовала признать незаконной регистрацию Путина на выборах', 'У Грудинина обнаружили два не до конца закрытых счета в Швейцарии и Австрии', 'Грудинин раскрыл историю происхождения дома в Испании', 'Путина зарегистрировали кандидатом в президенты', 'В Кремле отреагировали на слухи о голосовании Путина в Севастополе', 'Коммунистов вновь обвинили в незаконной агитации за Грудинина', 'ЦИК выявила обман со стороны Грудинина', 'Грудинин ответил на претензии ЦИК', 'Жириновский захотел сбросить ядерную бомбу на резиденцию Порошенко']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь объединим все выбранные тексты в один и посчитаем ветор для него. Сделаем это два раза для выбранных тематик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sporttext=' '.join([lenta.articles[lenta.findNewsByTitle(art)] for art in likesport])\n",
    "sportdict=lenta.getArticleDictionary(sporttext)\n",
    "electtext=' '.join([lenta.articles[lenta.findNewsByTitle(art)] for art in likeelect])\n",
    "electdict=lenta.getArticleDictionary(electtext)\n",
    "#print(sportdict)\n",
    "#print(electdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь отберем все статьи, косинусная мера которых превышает некоторый порог."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrs=0.4\n",
    "thre=0.5\n",
    "cosess=[lenta.articles[i].split('\\n-----\\n')[0] for i in range(len(lenta.dictionaries)) if cosineSimilarity(sportdict, lenta.dictionaries[i])>thrs]\n",
    "print(cosess)\n",
    "cosese=[lenta.articles[i].split('\\n-----\\n')[0] for i in range(len(lenta.dictionaries)) if cosineSimilarity(electdict, lenta.dictionaries[i])>thre]\n",
    "print(cosese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки загрузим новости за какой-то другой день."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenta_new=getNewsPaper()\n",
    "#lenta_new.getLentaPeriod(datetime.date(2018, 2, 15), datetime.date(2018, 2, 15))\n",
    "#lenta_new.saveArticles(\"lenta20180215.txt\")\n",
    "lenta_new.loadArticles(\"lenta20180215.txt\")\n",
    "lenta_new.calcArticleDictionaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь проверим какие новости будут находиться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrs_new = 0.3\n",
    "thre_new = 0.3\n",
    "cosess_new = [lenta_new.articles[i].split('\\n-----\\n')[0] for i in range(len(lenta_new.dictionaries)) if cosineSimilarity(sportdict, lenta_new.dictionaries[i])>thrs_new]\n",
    "print(cosess_new)\n",
    "cosese_new = [lenta_new.articles[i].split('\\n-----\\n')[0] for i in range(len(lenta_new.dictionaries)) if cosineSimilarity(electdict, lenta_new.dictionaries[i])>thre_new]\n",
    "print(cosese_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, метод нуждается в более точном подборе и корректировке параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо того, чтобы выделять самые частотные слова в одной статье, мы можем использовать несколько иную логику. Слово является важным, если оно часто встречается в <b>данной</b> статье. Однако если это слово часто встречается во всех статьях, скорее всего оно не значимо (так ведут себя предлоги, союзы, слова, относящиеся к особенностям авторского или принятого в данной области стиля). То есть наиболее характерными для данного документа будут слова и словосочетания, которые встречаются часто в данном докумемнте, но в малом количестве документов.<br>\n",
    "Для выделения таких слов будем использовать TfidfVectorizer, работающий по формуле tf*idf, где tf - term frequency, а idf - inverted document frequency. То есть в простейшем случае можно просто разделить частоту термина в документе на количество документов, в которых он встречается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=[' '.join(getMeaningfullWords(n)) for n in lenta.articles]\n",
    "\n",
    "tfCounter=TfidfVectorizer(ngram_range=(1,2), token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "\n",
    "analyze = tfCounter.build_analyzer()\n",
    "res=tfCounter.fit_transform(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_article=6\n",
    "\n",
    "res2=analyze(lenta.articles[id_article])\n",
    "tfs=list(set(res[id_article][0, tfCounter.vocabulary_.get(k)] for k in res2 if k in tfCounter.vocabulary_.keys()))\n",
    "tfs2=[k for k in tfs if k>np.average(tfs)]\n",
    "#tfs2=[k for k in tfs if k>np.average(tfs)+np.std(tfs)]\n",
    "print({w:res[id_article][0, tfCounter.vocabulary_[w]] for w in res2 if w in tfCounter.vocabulary_.keys() and res[id_article][0, tfCounter.vocabulary_[w]] in tfs2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}